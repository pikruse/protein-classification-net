{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pkr/miniforge3/lib/python3.10/site-packages/esm/utils/structure/protein_structure.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled=False)\n",
      "/home/pkr/miniforge3/lib/python3.10/site-packages/esm/utils/structure/protein_structure.py:164: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled=False)\n",
      "/home/pkr/miniforge3/lib/python3.10/site-packages/esm/utils/structure/protein_structure.py:213: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled=False)\n"
     ]
    }
   ],
   "source": [
    "# load necessary packages\n",
    "from huggingface_hub import login\n",
    "from esm.models.esm3 import ESM3\n",
    "from esm.sdk.api import ESM3InferenceClient, ESMProtein, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pkr/miniforge3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe78ab6930848f59084d0c5298cb84c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 22 files:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pkr/miniforge3/lib/python3.10/site-packages/esm/pretrained.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n"
     ]
    }
   ],
   "source": [
    "# This will download the model weights and instantiate the model on your machine.\n",
    "model: ESM3InferenceClient = ESM3.from_pretrained(\"esm3_sm_open_v1\").to(\"cuda\") # or \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  7.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ESMProteinError(error_msg='Cannot sample sequence when input has no masks.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a completion for a partial Carbonic Anhydrase (2vvb)\n",
    "prompt = \"\"\n",
    "\n",
    "# create ESMProtein object with the sequence\n",
    "protein = ESMProtein(sequence=prompt)\n",
    "\n",
    "# Generate the sequence, then the structure. This will iteratively unmask the sequence track.\n",
    "protein = model.generate(protein, GenerationConfig(track=\"sequence\", num_steps=8, temperature=0.7))\n",
    "protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 19.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ESMProtein(sequence='ADASDSADSDAGDQATSLRILNNGHAFNVEFDDSQDKAVLKGGPLDGTYRLIQFHFHWGSLDGQGSEHTVDKKKYAAELHLVHWNTKYGDFGKAVQQPDGLAVLGIFLKVGSAKPGLQKVVDVLDSIKTKGKSADFTNFDPRGLLPESLDYWTYPGSLTTPPCTESVIWTVFKTPVEVSQSQVERSRSILTMDGPSGPNGRTGNFRPVQPLQNPLPGAVRX', secondary_structure=None, sasa=None, function_annotations=None, coordinates=tensor([[[  3.6075, -27.3798,   9.7120],\n",
       "         [  2.5275, -27.2766,   8.7363],\n",
       "         [  2.0837, -28.6558,   8.2601],\n",
       "         ...,\n",
       "         [     inf,      inf,      inf],\n",
       "         [     inf,      inf,      inf],\n",
       "         [     inf,      inf,      inf]],\n",
       "\n",
       "        [[  2.2180, -28.5714,   8.1195],\n",
       "         [  1.9468, -29.6865,   7.2183],\n",
       "         [  1.1628, -29.2254,   5.9941],\n",
       "         ...,\n",
       "         [     inf,      inf,      inf],\n",
       "         [     inf,      inf,      inf],\n",
       "         [     inf,      inf,      inf]],\n",
       "\n",
       "        [[  1.5749, -28.8617,   5.6425],\n",
       "         [  1.3004, -28.9610,   4.2128],\n",
       "         [ -0.1998, -29.0184,   3.9442],\n",
       "         ...,\n",
       "         [     inf,      inf,      inf],\n",
       "         [     inf,      inf,      inf],\n",
       "         [     inf,      inf,      inf]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-19.2315,  16.3172, -11.3564],\n",
       "         [-19.1246,  17.0983, -12.5842],\n",
       "         [-19.4578,  16.2491, -13.8064],\n",
       "         ...,\n",
       "         [     inf,      inf,      inf],\n",
       "         [     inf,      inf,      inf],\n",
       "         [     inf,      inf,      inf]],\n",
       "\n",
       "        [[-19.4697,  16.2976, -13.7765],\n",
       "         [-19.4630,  15.6380, -15.0781],\n",
       "         [-20.3990,  16.3415, -16.0553],\n",
       "         ...,\n",
       "         [     inf,      inf,      inf],\n",
       "         [     inf,      inf,      inf],\n",
       "         [     inf,      inf,      inf]],\n",
       "\n",
       "        [[-20.2841,  16.0887, -16.1265],\n",
       "         [-20.5439,  17.0012, -17.2351],\n",
       "         [-21.8357,  16.6339, -17.9577],\n",
       "         ...,\n",
       "         [     inf,      inf,      inf],\n",
       "         [     inf,      inf,      inf],\n",
       "         [     inf,      inf,      inf]]]), plddt=tensor([0.2912, 0.3462, 0.3847, 0.3681, 0.3054, 0.3019, 0.2886, 0.2676, 0.2670,\n",
       "        0.2551, 0.2940, 0.3024, 0.3002, 0.3568, 0.4072, 0.4454, 0.4795, 0.5665,\n",
       "        0.5734, 0.6136, 0.5924, 0.5433, 0.5272, 0.5141, 0.5167, 0.5017, 0.5338,\n",
       "        0.5060, 0.5416, 0.5130, 0.5497, 0.5333, 0.5508, 0.5639, 0.6151, 0.6819,\n",
       "        0.6777, 0.7423, 0.7982, 0.8257, 0.8301, 0.7872, 0.7590, 0.7794, 0.8414,\n",
       "        0.8618, 0.8814, 0.8853, 0.8900, 0.8391, 0.8103, 0.7873, 0.7764, 0.7928,\n",
       "        0.7947, 0.8050, 0.7814, 0.7660, 0.7352, 0.6942, 0.7251, 0.7362, 0.7729,\n",
       "        0.7483, 0.7695, 0.7723, 0.7159, 0.7432, 0.7511, 0.7564, 0.7443, 0.7494,\n",
       "        0.8198, 0.8078, 0.8497, 0.8515, 0.8491, 0.8493, 0.8621, 0.8574, 0.8509,\n",
       "        0.8683, 0.8754, 0.9057, 0.9235, 0.8989, 0.9233, 0.9334, 0.8933, 0.8806,\n",
       "        0.8780, 0.8988, 0.9409, 0.9393, 0.9279, 0.9426, 0.9473, 0.9258, 0.9254,\n",
       "        0.9439, 0.9323, 0.9244, 0.9053, 0.8981, 0.8916, 0.9029, 0.8954, 0.8921,\n",
       "        0.8907, 0.8724, 0.8818, 0.8775, 0.8633, 0.8593, 0.8427, 0.8242, 0.8673,\n",
       "        0.8594, 0.8454, 0.8431, 0.8442, 0.8336, 0.8178, 0.8066, 0.7903, 0.7705,\n",
       "        0.7361, 0.7177, 0.6686, 0.6363, 0.6267, 0.6708, 0.7025, 0.7118, 0.7166,\n",
       "        0.7186, 0.6897, 0.6966, 0.7314, 0.7659, 0.7881, 0.7854, 0.8119, 0.8582,\n",
       "        0.8608, 0.8835, 0.8678, 0.8805, 0.8510, 0.8666, 0.8721, 0.8454, 0.8511,\n",
       "        0.8325, 0.8407, 0.8247, 0.8187, 0.8172, 0.7468, 0.7051, 0.7299, 0.7994,\n",
       "        0.8112, 0.8819, 0.8816, 0.9179, 0.9045, 0.8991, 0.8852, 0.8928, 0.8824,\n",
       "        0.8952, 0.8937, 0.9101, 0.9180, 0.9209, 0.9154, 0.9108, 0.8801, 0.8444,\n",
       "        0.8297, 0.8500, 0.8351, 0.8102, 0.8052, 0.7902, 0.7496, 0.7201, 0.6880,\n",
       "        0.6046, 0.5355, 0.4719, 0.4286, 0.4096, 0.3686, 0.3842, 0.3650, 0.3527,\n",
       "        0.3240, 0.3538, 0.3320, 0.3671, 0.3026, 0.2887, 0.3064, 0.2995, 0.3859,\n",
       "        0.3743, 0.4032, 0.4915, 0.4257, 0.4475, 0.4876, 0.4086, 0.4098, 0.4085,\n",
       "        0.3211, 0.3812, 0.4119, 0.4108, 0.4415]), ptm=tensor([0.7120], device='cuda:0'), potential_sequence_of_concern=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can show the predicted structure for the generated sequence.\n",
    "protein = model.generate(protein, GenerationConfig(track=\"structure\", num_steps=8))\n",
    "protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESM3(\n",
      "  (encoder): EncodeInputs(\n",
      "    (sequence_embed): Embedding(64, 1536)\n",
      "    (plddt_projection): Linear(in_features=16, out_features=1536, bias=True)\n",
      "    (structure_per_res_plddt_projection): Linear(in_features=16, out_features=1536, bias=True)\n",
      "    (structure_tokens_embed): Embedding(4101, 1536)\n",
      "    (ss8_embed): Embedding(11, 1536)\n",
      "    (sasa_embed): Embedding(19, 1536)\n",
      "    (function_embed): ModuleList(\n",
      "      (0-7): 8 x Embedding(260, 192, padding_idx=0)\n",
      "    )\n",
      "    (residue_embed): EmbeddingBag(1478, 1536, mode='sum', padding_idx=0)\n",
      "  )\n",
      "  (transformer): TransformerStack(\n",
      "    (blocks): ModuleList(\n",
      "      (0): UnifiedTransformerBlock(\n",
      "        (attn): MultiHeadAttention(\n",
      "          (layernorm_qkv): Sequential(\n",
      "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=1536, out_features=4608, bias=False)\n",
      "          )\n",
      "          (out_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "          (q_ln): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          (k_ln): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          (rotary): RotaryEmbedding()\n",
      "        )\n",
      "        (geom_attn): GeometricReasoningOriginalImpl(\n",
      "          (s_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          (proj): Linear(in_features=1536, out_features=3840, bias=False)\n",
      "          (out_proj): Linear(in_features=768, out_features=1536, bias=False)\n",
      "        )\n",
      "        (ffn): Sequential(\n",
      "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=1536, out_features=8192, bias=False)\n",
      "          (2): SwiGLU()\n",
      "          (3): Linear(in_features=4096, out_features=1536, bias=False)\n",
      "        )\n",
      "      )\n",
      "      (1-47): 47 x UnifiedTransformerBlock(\n",
      "        (attn): MultiHeadAttention(\n",
      "          (layernorm_qkv): Sequential(\n",
      "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=1536, out_features=4608, bias=False)\n",
      "          )\n",
      "          (out_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "          (q_ln): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          (k_ln): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          (rotary): RotaryEmbedding()\n",
      "        )\n",
      "        (ffn): Sequential(\n",
      "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=1536, out_features=8192, bias=False)\n",
      "          (2): SwiGLU()\n",
      "          (3): Linear(in_features=4096, out_features=1536, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (output_heads): OutputHeads(\n",
      "    (sequence_head): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=1536, out_features=64, bias=True)\n",
      "    )\n",
      "    (structure_head): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=1536, out_features=4096, bias=True)\n",
      "    )\n",
      "    (ss8_head): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=1536, out_features=11, bias=True)\n",
      "    )\n",
      "    (sasa_head): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=1536, out_features=19, bias=True)\n",
      "    )\n",
      "    (function_head): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=1536, out_features=2080, bias=True)\n",
      "    )\n",
      "    (residue_head): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=1536, out_features=1478, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class OccupancyESM: \n",
    "    def __init__(self, model_name: str,\n",
    "                 layers: list = [256],\n",
    "                 activation = torch.nn.ReLU()\n",
    "                 ):\n",
    "        self.model = ESM3.from_pretrained(model_name).to(\"cuda\")\n",
    "        self.num_layers = len(layers)\n",
    "        self.        \n",
    "\n",
    "\n",
    "    def forward(self, input_sequence: str):\n",
    "        x = ESMProtein(sequence=input_sequence)\n",
    "\n",
    "   \n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
