{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Dataset' from 'torch.nn.utils' (/home/pkr/miniconda3/envs/protein/lib/python3.11/site-packages/torch/nn/utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m login\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mesm\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Dataset' from 'torch.nn.utils' (/home/pkr/miniconda3/envs/protein/lib/python3.11/site-packages/torch/nn/utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "# load necessary packages\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import Dataset, DataLoader\n",
    "from huggingface_hub import login\n",
    "\n",
    "import esm\n",
    "from esm.models.esm3 import ESM3\n",
    "from esm.sdk.api import ESM3InferenceClient, ESMProtein, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pkr/miniconda3/envs/protein/lib/python3.11/site-packages/esm/pretrained.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n"
     ]
    }
   ],
   "source": [
    "# This will download the model weights and instantiate the model on your machine.\n",
    "model = ESM3.from_pretrained(\"esm3_sm_open_v1\").to(\"cuda\") # or \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ESM3.forward() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# create ESMProtein object with the sequence\u001b[39;00m\n\u001b[1;32m      5\u001b[0m protein \u001b[38;5;241m=\u001b[39m ESMProtein(sequence\u001b[38;5;241m=\u001b[39mprompt)\n\u001b[0;32m----> 7\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: ESM3.forward() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# Generate a completion for a partial Carbonic Anhydrase (2vvb)\n",
    "prompt = \"___________________________________________________DQATSLRILNNGHAFNVEFDDSQDKAVLKGGPLDGTYRLIQFHFHWGSLDGQGSEHTVDKKKYAAELHLVHWNTKYGDFGKAVQQPDGLAVLGIFLKVGSAKPGLQKVVDVLDSIKTKGKSADFTNFDPRGLLPESLDYWTYPGSLTTPP___________________________________________________________\"\n",
    "\n",
    "# create ESMProtein object with the sequence\n",
    "protein = ESMProtein(sequence=prompt)\n",
    "\n",
    "out = model.forward(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input must be an ESMProtein or ESMProteinTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# We can show the predicted structure for the generated sequence.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m protein \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotein\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGenerationConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstructure\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m protein\n",
      "File \u001b[0;32m~/miniconda3/envs/protein/lib/python3.11/site-packages/esm/models/esm3.py:397\u001b[0m, in \u001b[0;36mESM3.generate\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: ProteinType, config: GenerationConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ProteinType:\n\u001b[1;32m    396\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrap around batched generation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 397\u001b[0m     proteins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(proteins) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m proteins[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/protein/lib/python3.11/site-packages/esm/models/esm3.py:430\u001b[0m, in \u001b[0;36mESM3.batch_generate\u001b[0;34m(self, inputs, configs)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m iterative_sampling_tokens(\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    425\u001b[0m         inputs,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    426\u001b[0m         configs,\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizers,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    428\u001b[0m     )\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 430\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput must be an ESMProtein or ESMProteinTensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Input must be an ESMProtein or ESMProteinTensor"
     ]
    }
   ],
   "source": [
    "# We can show the predicted structure for the generated sequence.\n",
    "protein = model.generate(protein, GenerationConfig(track=\"structure\", num_steps=8))\n",
    "protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESM3(\n",
      "  (encoder): EncodeInputs(\n",
      "    (sequence_embed): Embedding(64, 1536)\n",
      "    (plddt_projection): Linear(in_features=16, out_features=1536, bias=True)\n",
      "    (structure_per_res_plddt_projection): Linear(in_features=16, out_features=1536, bias=True)\n",
      "    (structure_tokens_embed): Embedding(4101, 1536)\n",
      "    (ss8_embed): Embedding(11, 1536)\n",
      "    (sasa_embed): Embedding(19, 1536)\n",
      "    (function_embed): ModuleList(\n",
      "      (0-7): 8 x Embedding(260, 192, padding_idx=0)\n",
      "    )\n",
      "    (residue_embed): EmbeddingBag(1478, 1536, mode='sum', padding_idx=0)\n",
      "  )\n",
      "  (transformer): TransformerStack(\n",
      "    (blocks): ModuleList(\n",
      "      (0): UnifiedTransformerBlock(\n",
      "        (attn): MultiHeadAttention(\n",
      "          (layernorm_qkv): Sequential(\n",
      "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=1536, out_features=4608, bias=False)\n",
      "          )\n",
      "          (out_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "          (q_ln): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          (k_ln): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          (rotary): RotaryEmbedding()\n",
      "        )\n",
      "        (geom_attn): GeometricReasoningOriginalImpl(\n",
      "          (s_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          (proj): Linear(in_features=1536, out_features=3840, bias=False)\n",
      "          (out_proj): Linear(in_features=768, out_features=1536, bias=False)\n",
      "        )\n",
      "        (ffn): Sequential(\n",
      "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=1536, out_features=8192, bias=False)\n",
      "          (2): SwiGLU()\n",
      "          (3): Linear(in_features=4096, out_features=1536, bias=False)\n",
      "        )\n",
      "      )\n",
      "      (1-47): 47 x UnifiedTransformerBlock(\n",
      "        (attn): MultiHeadAttention(\n",
      "          (layernorm_qkv): Sequential(\n",
      "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=1536, out_features=4608, bias=False)\n",
      "          )\n",
      "          (out_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "          (q_ln): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          (k_ln): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          (rotary): RotaryEmbedding()\n",
      "        )\n",
      "        (ffn): Sequential(\n",
      "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=1536, out_features=8192, bias=False)\n",
      "          (2): SwiGLU()\n",
      "          (3): Linear(in_features=4096, out_features=1536, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (output_heads): OutputHeads(\n",
      "    (sequence_head): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=1536, out_features=64, bias=True)\n",
      "    )\n",
      "    (structure_head): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=1536, out_features=4096, bias=True)\n",
      "    )\n",
      "    (ss8_head): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=1536, out_features=11, bias=True)\n",
      "    )\n",
      "    (sasa_head): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=1536, out_features=19, bias=True)\n",
      "    )\n",
      "    (function_head): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=1536, out_features=2080, bias=True)\n",
      "    )\n",
      "    (residue_head): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=1536, out_features=1478, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class OccupancyESM: \n",
    "    def __init__(self, model_name: str,\n",
    "                 layers: list = [256],\n",
    "                 activation = torch.nn.ReLU()\n",
    "                 ):\n",
    "        self.model = ESM3.from_pretrained(model_name).to(\"cuda\")\n",
    "        self.num_layers = len(layers)\n",
    "        self.        \n",
    "\n",
    "\n",
    "    def forward(self, input_sequence: str):\n",
    "        x = ESMProtein(sequence=input_sequence)\n",
    "\n",
    "   \n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
